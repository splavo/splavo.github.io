<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MNIST Neural Network Project</title>
    <style>
        body {
            font-family: 'Consolas', 'Monaco', 'Andale Mono', 'Courier New', monospace;
            background-color: #1e1e1e;
            color: #d4d4d4;
            margin: 20px;
        }

        h1 {
            color: #4ec9b0;
        }

        p {
            color: #d4d4d4;
        }

        code {
            display: block;
            white-space: pre-wrap;
            background-color: #2d2d2d;
            color: #d4d4d4;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
            line-height: 1.5;
        }
    </style>
</head>
<body>

<h1>MNIST Neural Network Project</h1>

<p>Below is the code for my MNIST neural network project:</p>

<code>
    import numpy as np
    import matplotlib.pyplot as plt

    from network import Network
    from fc_layer import FCLayer
    from activation_layer import ActivationLayer
    from activations import tanh, tanh_prime
    from losses import mse, mse_prime

    from keras.datasets import mnist
    from keras.utils import np_utils

    # load MNIST from server
    (x_train, y_train), (x_test, y_test) = mnist.load_data()

    # training data : 60000 samples
    # reshape and normalize input data
    x_train = x_train.reshape(x_train.shape[0], 1, 28*28)
    x_train = x_train.astype('float32')
    x_train /= 255
    # encode output which is a number in range [0,9] into a vector of size 10
    # e.g. number 3 will become [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]
    y_train = np_utils.to_categorical(y_train)

    # same for test data : 10000 samples
    x_test = x_test.reshape(x_test.shape[0], 1, 28*28)
    x_test = x_test.astype('float32')
    x_test /= 255
    y_test = np_utils.to_categorical(y_test)

    # Network
    net = Network()
    net.add(FCLayer(28*28, 100))                # input_shape=(1, 28*28)    ;   output_shape=(1, 100)
    net.add(ActivationLayer(tanh, tanh_prime))
    net.add(FCLayer(100, 50))                   # input_shape=(1, 100)      ;   output_shape=(1, 50)
    net.add(ActivationLayer(tanh, tanh_prime))
    net.add(FCLayer(50, 10))                    # input_shape=(1, 50)       ;   output_shape=(1, 10)
    net.add(ActivationLayer(tanh, tanh_prime))


    # train on 1000 samples
    # as we didn't implemented mini-batch GD, training will be pretty slow if we update at each iteration on 60000 samples...
    net.use(mse, mse_prime)
    net.fit(x_train[0:1000], y_train[0:1000], epochs=35, learning_rate=0.1)

    # test on 3 samples
    out = net.predict(x_test[0:3])
    print("\n")
    print("predicted values : ")
    print(out, end="\n")
    print("true values : ")
    print(y_test[0:3])



    def predict(network, input):
        output = input
        for layer in network.layers:
            output = layer.forward_propagation(output)
        return output

    # ratio = sum([np.argmax(y) == np.argmax(predict(net, x)) for x, y in zip(x_test, y_test)]) / len(x_test)
    # error = sum([mse(y, predict(net, x)) for x, y in zip(x_test, y_test)]) / len(x_test)
    # print('ratio: %.2f' % ratio)
    # print('mse: %.4f' % error)

    samples = 10
    for test, true in zip(x_test[100:samples+100], y_test[100:samples+100]):
        image = np.reshape(test, (28, 28))
        plt.imshow(image, cmap='binary')
        pred = predict(net, test)[0]
        idx = np.argmax(pred)
        idx_true = np.argmax(true)
        text = 'pred: %s, prob: %.2f, true: %d' % (idx, pred[idx], idx_true)
        # print('pred: %s, prob: %.2f, true: %d' % (idx, pred[idx], idx_true))
        plt.text(2.5,-2, text, fontsize = 17)
        plt.show()

</code>

</body>
</html>
