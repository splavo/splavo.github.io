<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MNIST Neural Network Project</title>
    <style>
        body {
            font-family: 'Consolas', 'Monaco', 'Andale Mono', 'Courier New', monospace;
            background-color: #1e1e1e;
            color: #d4d4d4;
            margin: 20px;
        }

        h1 {
            color: #4ec9b0;
        }

        p {
            color: #d4d4d4;
        }

        code {
            display: block;
            white-space: pre-wrap;
            background-color: #2d2d2d;
            color: #d4d4d4;
            padding: 10px;
            border-radius: 5px;
            overflow-x: auto;
            line-height: 1.5;
        }
    </style>
</head>
<body>

<h1>MNIST Neural Network Project</h1>

<p>Below is the code for my MNIST neural network project:</p>

<code>

    import numpy as np
    import nnfs
    from nnfs.datasets import spiral_data
    np.random.seed(0)
    
    nnfs.init()
    
    class Layer_Dense:
        def __init__(self, n_inputs, n_neurons):
            self.weights = 0.1 * np.random.randn(n_inputs,n_neurons)
            self.biases = np.zeros((1,))
            
        def forward(self, inputs):
            self.output = np.dot(inputs, self.weights) + self.biases
    
    
    class Activation_ReLU:
        def forward(self, inputs):
            self.output = np.maximum(0,inputs)
    
    
    class Activation_Softmax:
        def forward(self, inputs):
            exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))
            probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)
            self.output = probabilities
    
    
    class Loss:
        def calculate(self, output, y): #output from model, y = intended target values
            sample_losses = self.forward(output, y)
            data_loss = np.mean(sample_losses)
            return data_loss
        
    class Loss_CategoricalEntropy(Loss):
        def forward(self, y_pred, y_true):
            samples = len(y_pred)
            y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7)
    
            if len(y_true.shape) == 1: #if scalar values passed
                correct_confidences = y_pred_clipped[range(samples), y_true]
    
            elif len(y_true.shape) == 2: #if one hot encoded values passed
                correct_confidences = np.sum(y_pred_clipped * y_true, axis=1)
            
            negative_log_likelihoods = -np.log(correct_confidences)
            return negative_log_likelihoods
    
    X, y = spiral_data(samples=100, classes=3)
    
    dense1 = Layer_Dense(2,3)
    activation1 = Activation_ReLU()
    
    dense2 = Layer_Dense(3,3)
    activation2 = Activation_Softmax()
    
    dense1.forward(X)
    activation1.forward(dense1.output)
    
    dense2.forward(activation1.output)
    activation2.forward(dense2.output)
    
    print(activation2.output[:5])
    
    loss_function = Loss_CategoricalEntropy()
    loss = loss_function.calculate(activation2.output, y)
    print("Loss: ", loss)
    
</code>

</body>
</html>
